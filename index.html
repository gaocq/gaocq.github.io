<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Chenqiang Gao - Sun Yat-sen University (Shenzhen)</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="style.css?v=1.1">

</head>

<body>

<!-- Header + Nav -->
<header class="site-header">
  <div class="container nav-row">
    
    <div class="site-identity">
      <a class="brand" href="index.html">Chenqiang Gao（高陈强）</a>
      
      <div class="brand-sub">Professor, Sun Yat-sen University (Shenzhen Campus).</div>
      
      <div class="links-inline">
        <a href="https://scholar.google.com/citations?user=YiVDoL0AAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer">Google Scholar</a>
        <a href="https://github.com/gaocq" target="_blank" rel="noopener noreferrer">GitHub</a>
        <a href="mailto:gaochq6@mail.sysu.edu.cn">gaochq6@mail.sysu.edu.cn</a>
      </div>
    </div>
    
    <nav class="site-nav">
      <a href="index.html">Home</a>
      <a href="publications.html">Publications</a>
      <a href="contact.html">Contact</a>
    </nav>

  </div>
</header>


<!-- INTRO BLOCK (photo floats, text wraps) -->
<div class="main-content">
  <div class="intro-block">
    <div class="intro-photo">
      <img src="imgs/gaocq-photo.jpg" alt="Chenqiang Gao"  >
      <div class="intro-name">Chenqiang Gao</div>
    </div>

    <p>
    I received my B.S. in Computer Science from <a href="https://en.cug.edu.cn/" target="_blank" rel="noopener noreferrer"> China University of Geosciences (CUG)</a>, Wuhan, in 2004,
    and my Ph.D. in 2009 from the Institute of Image Recognition &amp; Artificial Intelligence,
    <a href="https://english.hust.edu.cn/" target="_blank" rel="noopener noreferrer">Huazhong University of Science and Technology (HUST)</a>, advised by Prof. Jinwen Tian. 
    </p>

    <p>
    In 2009, I returned to my hometown and joined <a href="https://english.cqupt.edu.cn/" target="_blank" rel="noopener noreferrer"> Chongqing University of Posts and Telecommunications (CQUPT)</a>. In 2010,
    I spent one year teaching middle school students in the Daguan town. In 2012, I joined the Informedia Group at <a href="https://www.cmu.edu/" target="_blank" rel="noopener noreferrer">Carnegie Mellon University (CMU)</a>,
    working with <a href="https://www.cs.cmu.edu/~alex/" target="_blank" rel="noopener noreferrer">Prof. Alexander G. Hauptmann</a> as <a href="https://www.cs.cmu.edu/~gaocq/" target="_blank" rel="noopener noreferrer"> a postdoctoral fellow </a>. 
    </p>

    <p>
    From 2014 to 2023, I was a full professor at CQUPT, leading the Intelligent Multimedia Research Center
    and the Chongqing Key Laboratory of Signal and Information Processing. In September 2023, I joined
    <a href="https://ise.sysu.edu.cn/" target="_blank" rel="noopener noreferrer">the School of Intelligent Systems Engineering </a> at <a href="https://www.sysu.edu.cn/sysuen/" target="_blank" rel="noopener noreferrer">Sun Yat-sen University (Shenzhen Campus)</a> as a full
    professor, and I continue close collaboration with CQUPT, including joint supervision of Master's and
    PhD students. 
    </p>

    <p>
    Official faculty pages:
    <a href="https://ise.sysu.edu.cn/teacher/GaoChenqiang" target="_blank" rel="noopener noreferrer">SYSU</a>,
    <a href="https://faculty.cqupt.edu.cn/gaocq/zh_CN/index.htm" target="_blank" rel="noopener noreferrer">CQUPT</a>. 
    </p>

    <p>
      <strong>
        I am recruiting highly self-motivated Postdocs, PhD / Master students and long-term research interns,
        please read the recruiting note here: [<a href="https://docs.qq.com/doc/DUHhkZUxnR1Z2cXpU" target="_blank" rel="noopener noreferrer"> 招生说明 </a>].
      </strong>
    </p>

  </div>

  <div class="clearfix-after-intro"></div>

  <hr>

  <!-- RESEARCH INTERESTS -->
  <h2>Research Interests</h2>
  <ul>
    <li>
      <strong>Multimodal perception & fusion</strong>: robust detection and recognition under challenging conditions using infrared, visible, LiDAR point clouds, SAR, and their fusion (e.g., IR+visible in low light, visible+navigation radar in real scenes).
    </li>
    <li>
      <strong>Infrared & 3D scene understanding</strong>: infrared small-target detection, scalable infrared foundation models for low-SNR tasks, and LiDAR-based 3D object detection and scene perception.
    </li>
    <li>
      <strong>Controllable generation for perception & planning</strong>: diffusion-based translation and synthesis of sensor data (e.g., visible→infrared) and condition-controlled image/video generation to support detection, tracking, and planning.
    </li>
    <li>
      <strong>Medical & industrial vision</strong>: dental/tooth segmentation from X-ray, CBCT, and 3D oral scans, as well as industrial surface defect inspection.
    </li>
    <li>
      <strong>Video understanding & embodied multimodal intelligence</strong>: long-duration video understanding, behavior/event analysis in the wild, and coupling all lines with multimodal large models and perception–action loops for robots/agents with reasoning-style supervision.
    </li>
  </ul>




  <!-- <h2>Research Interests</h2>
  <p>
  Our group focuses on multimodal perception and long-horizon scene understanding under real
  deployment constraints. First, we study target detection and recognition in challenging sensing
  conditions using individual modalities such as infrared, visible light, LiDAR point clouds, and
  SAR. We have done extensive work on infrared small target detection, and we are building and scaling
  infrared foundation models aimed at becoming general backbones for downstream infrared tasks at
  low signal-to-noise. We also work on 3D point-cloud object detection and robust perception from
  LiDAR. In parallel, we push multi-sensor fusion: for example, joint infrared + visible detection
  in low-light and adverse environments, and ongoing work on visible + navigation radar fusion for
  reliable target perception in real scenes rather than controlled lab setups.
  </p>

  <p>
  Beyond perception, we are moving toward controllable generation, temporal reasoning, and embodied
  intelligence. We use diffusion-based generative models to translate and synthesize sensor data
  (e.g., generating infrared imagery from visible imagery), and we are extending this toward
  physically grounded, condition-controlled image/video generation for downstream detection,
  tracking, and planning. We also work on medical and industrial vision — including dental/tooth
  segmentation from X-ray, CBCT, and 3D oral scan data, and defect inspection in industrial
  surfaces that has already drawn public attention — and on video understanding in the wild:
  behavior/event detection in visible and infrared video, classroom activity analysis for education
  scenarios, and current work on long-duration video understanding. We are actively exploring
  embodied intelligence (robotic / agent perception-action loops). Going forward, all of these lines are being coupled with
  multimodal large models and reasoning-style supervision so that the system is not only "seeing"
  but also describing, deciding, and adapting to new environments with minimal human labeling.
  </p> -->

  <hr>

  <!-- TEACHING -->
  <h2>Teaching</h2>
  <ul>
    <li>"Video Technology" (Undergraduate)</li>
    <li>"Digital Image Processing" (Undergraduate)</li>
    <li>"Pattern Recognition and Machine Learning" (Graduate)</li>
  </ul>

  <hr>

</div>


</body>
</html>
